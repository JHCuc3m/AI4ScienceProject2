import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import os
import glob

EMBEDDING_DIR = '/Users/shreyasshrestha/Projects/Autoencoder/proteins/'
SEQUENCE_CSV = '/Users/shreyasshrestha/Projects/Autoencoder/Shrestha Autoencoder/sequences.csv'
MAX_SEQ_LENGTH = 500  #truncate all sequences to this length
NUM_AMINO_ACIDS = 21   #20 standard + 1 for a padding/start token

aa_to_idx = {
    'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5, 'Q': 6, 'E': 7, 'G': 8, 'H': 9,
    'I': 10, 'L': 11, 'K': 12, 'M': 13, 'F': 14, 'P': 15, 'S': 16, 'T': 17,
    'W': 18, 'Y': 19, 'V': 20, '_': 0 # Padding token
}
VALID_AA = set(aa_to_idx.keys()) - {'_'}


def load_and_prepare_data(embedding_dir, sequence_path):
    
    #Loads embeddings from .npz files, sequences from CSV, matches them, and tokenizes the sequences.
    
    try:
        #load embeddings
        embedding_files = sorted(glob.glob(os.path.join(embedding_dir, '*.npz')))
        if not embedding_files:
            raise FileNotFoundError(f"No .npz files found in '{embedding_dir}'")

        all_embeddings_list = []
        embedding_filenames = []
        for file_path in embedding_files:
            with np.load(file_path) as data:
                if 'pair' in data:
                    pair_embedding = data['pair']
                    protein_vector = np.mean(pair_embedding, axis=(0, 1))
                    all_embeddings_list.append(protein_vector)
                    embedding_filenames.append(os.path.basename(file_path))
        
        X_embeddings = torch.from_numpy(np.array(all_embeddings_list)).float()

        sequence_map = {}
        with open(sequence_path, 'r') as f:
            # Skip the header line
            next(f)
            for line in f:
                # Split the line only on the first comma to handle extra commas in the sequence
                parts = line.strip().split(',', 1)
                if len(parts) == 2:
                    filename, sequence = parts
                    sequence_map[filename] = sequence
        
        print(f"Loaded {len(X_embeddings)} embeddings and {len(sequence_map)} sequences.")

    except FileNotFoundError as e:
        print(f"Error: Could not find a required file. {e}")
        return None, None
    except Exception as e:
        print(f"An error occurred during data loading: {e}")
        return None, None

    tokenized_sequences = []
    # ensure filenames lime up
    for filename in embedding_filenames:
        sequence = sequence_map.get(filename, "")

        cleaned_sequence = ''.join([char for char in sequence if char in VALID_AA])
        truncated_sequence = cleaned_sequence[:MAX_SEQ_LENGTH]
        tokens = [aa_to_idx.get(aa, 0) for aa in truncated_sequence] # Use .get for safety
        padding_length = MAX_SEQ_LENGTH - len(tokens)
        padded_tokens = tokens + [0] * padding_length
        
        tokenized_sequences.append(padded_tokens)

    Y_sequences = torch.tensor(tokenized_sequences, dtype=torch.long)
    
    return X_embeddings, Y_sequences


#loading data
X_embeddings, Y_sequences = load_and_prepare_data(EMBEDDING_DIR, SEQUENCE_CSV)

if X_embeddings is None:
    exit() 


class InverseFoldingModel(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers):
        super(InverseFoldingModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        self.embedding_projection = nn.Linear(embedding_dim, hidden_dim)
        self.aa_embedding = nn.Embedding(output_dim, hidden_dim)
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, embedding, target_sequence=None):
        batch_size = embedding.shape[0]
        h0 = self.embedding_projection(embedding).unsqueeze(0).repeat(self.n_layers, 1, 1)
        c0 = torch.zeros_like(h0)
        hidden = (h0, c0)
        input_token = torch.zeros(batch_size, 1, dtype=torch.long).to(embedding.device)
        outputs = []

        for t in range(MAX_SEQ_LENGTH):
            embedded_token = self.aa_embedding(input_token)
            lstm_out, hidden = self.lstm(embedded_token, hidden)
            output = self.fc(lstm_out)
            outputs.append(output)
            
            if self.training and target_sequence is not None:
                input_token = target_sequence[:, t].unsqueeze(1)
            else:
                predicted_idx = output.argmax(dim=2)
                input_token = predicted_idx
        
        return torch.cat(outputs, dim=1)

#setup
model = InverseFoldingModel(embedding_dim=128, hidden_dim=256, output_dim=NUM_AMINO_ACIDS, n_layers=2)
criterion = nn.CrossEntropyLoss(ignore_index=0) # Ignore padding tokens (index 0)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

dataset = TensorDataset(X_embeddings, Y_sequences)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

#training
print("\nStarting training")
num_epochs = 200
model.train()
for epoch in range(num_epochs):
    for embeddings_batch, sequences_batch in loader:
        optimizer.zero_grad()
        outputs = model(embeddings_batch, sequences_batch)
        loss = criterion(outputs.view(-1, NUM_AMINO_ACIDS), sequences_batch.view(-1))
        loss.backward()
        optimizer.step()
        
    if (epoch + 1) % 20 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

print("Training done")

#embedding to amino acid sequence
model.eval()

idx_to_aa = {v: k for k, v in aa_to_idx.items()}

try:
    hybrid_embedding_path = 'hybrid_protein_embedding.pt'
    hybrid_embedding = torch.load(hybrid_embedding_path)
    
    with torch.no_grad():
        output_probabilities = model(hybrid_embedding)
        predicted_indices = torch.argmax(output_probabilities.squeeze(), dim=1)
        predicted_sequence = "".join([idx_to_aa.get(idx.item(), '?') for idx in predicted_indices])
        # Clean up padding tokens for a cleaner output
        final_sequence = predicted_sequence.replace('_', '')

        print(f"Generated sequence from '{hybrid_embedding_path}':")
        print(final_sequence)

except FileNotFoundError:
    print(f"Could not find '{hybrid_embedding_path}'. Skipping generation.")

