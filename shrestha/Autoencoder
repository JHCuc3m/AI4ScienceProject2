import numpy as np
import glob
import torch
import torch.nn as nn
from sklearn.model_selection import KFold
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

try:
    file_path = '/Users/shreyasshrestha/Projects/Autoencoder/proteins/*.npz'
    embedding_files = sorted(glob.glob(file_path))
    if not embedding_files:
        raise FileNotFoundError(f"No files found at path: {file_path}")

    all_protein_vectors_128d = []
    for file in embedding_files:
        with np.load(file) as data:
            if 'pair' in data:
                pair_embedding = data['pair']
                # create the averaged vector
                protein_vector = np.mean(pair_embedding, axis=(0, 1))
                all_protein_vectors_128d.append(protein_vector)

    X_128d = torch.from_numpy(np.array(all_protein_vectors_128d)).float()
    print(f"K-Fold shape: {X_128d.shape}")

except (FileNotFoundError, NameError) as e:
    print(f"Error loading data: {e}. Cannot proceed.")
    exit()

#start of the autoencoder
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(128, 96), nn.ReLU(),
            nn.Linear(96, 64)
        )
        self.decoder = nn.Sequential(
            nn.Linear(64, 96), nn.ReLU(),
            nn.Linear(96, 128)
        )
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

# k-fold cross validation
k_folds = 7
num_epochs = 400
batch_size = 4
results = {}

kfold = KFold(n_splits=k_folds, shuffle=True, random_state=73)

for fold, (train_ids, test_ids) in enumerate(kfold.split(X_128d)):
    print(f'FOLD {fold+1}/{k_folds}')

    model = Autoencoder()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()

    train_subset = X_128d[train_ids]
    test_subset = X_128d[test_ids]
    train_dataset = TensorDataset(train_subset, train_subset)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    model.train()
    for epoch in range(num_epochs):
        for inputs, _ in train_loader:
            reconstructed = model(inputs)
            loss = criterion(reconstructed, inputs)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    model.eval()
    with torch.no_grad():
        reconstructed_test = model(test_subset)
        test_loss = criterion(reconstructed_test, test_subset).item()
        
        reconstructed_train = model(train_subset)
        train_loss = criterion(reconstructed_train, train_subset).item()
        
        results[fold] = {'train_loss': train_loss, 'test_loss': test_loss}
        
        print(f'Final Training Loss for fold {fold+1}: {train_loss:.4f}')
        print(f'Test Loss for fold {fold+1}: {test_loss:.4f}')
        print(f'Generalization Gap: {test_loss - train_loss:.4f}')

#visualization start
avg_train_loss = np.mean([res['train_loss'] for res in results.values()])
avg_test_loss = np.mean([res['test_loss'] for res in results.values()])
# FIX: Define the avg_generalization_gap variable
avg_generalization_gap = avg_test_loss - avg_train_loss
print(f'Average Training Loss: {avg_train_loss:.4f}')
print(f'Average Test Loss: {avg_test_loss:.4f}')
print(f'Average Generalization Gap: {avg_generalization_gap:.4f}')

#train loss vs test loss
plt.figure(figsize=(12, 7))
fold_names = [f'Fold {i+1}' for i in results.keys()]
train_losses = [res['train_loss'] for res in results.values()]
test_losses = [res['test_loss'] for res in results.values()]
x = np.arange(len(fold_names))
width = 0.35
rects1 = plt.bar(x - width/2, train_losses, width, label='Train Loss', color='cornflowerblue')
rects2 = plt.bar(x + width/2, test_losses, width, label='Test Loss', color='salmon')
plt.ylabel('Loss (MSE)')
plt.title('Training vs. Test Loss per Fold')
plt.xticks(x, fold_names)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.text(0.95, 0.95, f'Avg. Generalization Gap: {avg_generalization_gap:.4f}',
         transform=plt.gca().transAxes, fontsize=12,
         verticalalignment='top', horizontalalignment='right',
         bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.5))
plt.tight_layout()
plt.show()


#test loss
plt.figure(figsize=(10, 6))
fold_names_orig = [f'Fold {i+1}' for i in results.keys()]
fold_losses_orig = [res['test_loss'] for res in results.values()]
plt.bar(fold_names_orig, fold_losses_orig, color='skyblue')
plt.axhline(y=avg_test_loss, color='r', linestyle='--', label=f'Average Loss: {avg_test_loss:.4f}')
plt.title('K-Fold Cross-Validation Test Loss')
plt.xlabel('Fold')
plt.ylabel('Test Loss (MSE)')
plt.legend()
plt.show()

#t-sne
model.eval()
with torch.no_grad():
    latent_embeddings = model.encoder(X_128d).numpy()

tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)
embeddings_2d = tsne.fit_transform(latent_embeddings)

plt.figure(figsize=(10, 8))
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7)
plt.title('2D Latent Space (t-SNE)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.grid(True)
plt.show()
